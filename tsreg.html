<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CPI Time Series Optimization Problem</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=EB+Garamond&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.6;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: #333;
        }
        
        .container {
            background: white;
            border-radius: 15px;
            padding: 30px;
            box-shadow: 0 20px 40px rgba(0,0,0,0.1);
        }
        
        h1 {
            color: #2c3e50;
            text-align: center;
            font-size: 2.5em;
            margin-bottom: 10px;
            background: linear-gradient(45deg, #e74c3c, #27ae60);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }
        
        .subtitle {
            text-align: center;
            color: #7f8c8d;
            font-size: 1.2em;
            margin-bottom: 30px;
        }
        
        h2 {
            color: #34495e;
            border-left: 4px solid #3498db;
            padding-left: 15px;
            margin-top: 40px;
        }
        
        h3 {
            color: #2980b9;
            margin-top: 25px;
        }
        
        .data-section {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin: 20px 0;
        }
        
        .data-card {
            background: linear-gradient(135deg, #f8f9fa, #e9ecef);
            border-radius: 10px;
            padding: 20px;
            border-left: 5px solid #e74c3c;
        }
        
        .data-card.changes {
            border-left-color: #27ae60;
        }
        
        .equation {
            background: #2c3e50;
            color: white;
            padding: 15px;
            border-radius: 8px;
            font-family: 'Courier New', monospace;
            text-align: center;
            margin: 15px 0;
            font-size: 1.1em;
        }
        
        .problem-box {
            background: linear-gradient(135deg, #ff6b6b, #feca57);
            color: white;
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
            font-weight: bold;
        }
        
        .solution-box {
            background: linear-gradient(135deg, #48cae4, #023e8a);
            color: white;
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
        }
        
        .optimizer-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }
        
        .optimizer-card {
            background: white;
            border: 2px solid #ecf0f1;
            border-radius: 10px;
            padding: 20px;
            transition: transform 0.2s;
        }
        
        .optimizer-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 25px rgba(0,0,0,0.1);
        }
        
        .optimizer-card h4 {
            color: #2980b9;
            margin-top: 0;
        }
        
        .code-block {
            background: #2d3748;
            color: #cbd5e0;
            padding: 15px;
            border-radius: 8px;
            overflow-x: auto;
            font-family: 'Courier New', monospace;
            margin: 15px 0;
        }
        
        .highlight {
            background: linear-gradient(120deg, #a8edea 0%, #fed6e3 100%);
            padding: 2px 6px;
            border-radius: 4px;
            font-weight: bold;
        }
        
        .results-section {
            background: linear-gradient(135deg, #667eea, #764ba2);
            color: white;
            padding: 25px;
            border-radius: 15px;
            margin: 30px 0;
        }
        
        .insight {
            background: #f39c12;
            color: white;
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
            border-left: 5px solid #e67e22;
        }
        
        .warning {
            background: #e74c3c;
            color: white;
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
            border-left: 5px solid #c0392b;
        }
        
        ul, ol {
            padding-left: 25px;
        }
        
        li {
            margin: 8px 0;
        }
        
        .emoji {
            font-size: 1.2em;
            margin-right: 8px;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>üö® CPI Time Series Optimization Disaster üö®</h1>
        <p class="subtitle">Why Data Scaling Matters in Machine Learning Optimization</p>
        
        <h2><span class="emoji">üìä</span>The Data: Two Faces of the Same Economic Story</h2>
        
        <p>We start with two CSV files containing Consumer Price Index (CPI) data - the same economic information presented in drastically different scales:</p>
        
        <div class="data-section">
            <div class="data-card">
                <h3>üî¥ CPI Levels (CPIAUCSL.csv)</h3>
                <ul>
                    <li><strong>Range:</strong> 78.0 to 307.5</li>
                    <li><strong>Scale:</strong> Large absolute numbers</li>
                    <li><strong>Example:</strong> 250.1, 251.3, 252.8...</li>
                    <li><strong>Problem:</strong> Creates numerical chaos</li>
                </ul>
            </div>
            
            <div class="data-card changes">
                <h3>üü¢ CPI Changes (CPIAUCSL_PCH.csv)</h3>
                <ul>
                    <li><strong>Range:</strong> -2.1% to +1.4%</li>
                    <li><strong>Scale:</strong> Small percentage changes</li>
                    <li><strong>Example:</strong> 0.3%, -0.1%, 0.7%...</li>
                    <li><strong>Problem:</strong> Well-behaved optimization</li>
                </ul>
            </div>
        </div>
        
        <div class="insight">
            <strong>üí° Key Insight:</strong> These datasets contain identical economic information! CPI changes are just the month-over-month percentage changes of CPI levels. Yet they create completely different optimization landscapes.
        </div>
        
        <h2><span class="emoji">üî¨</span>The Mathematical Setup</h2>
        
        <p>We create an autoregressive time series regression for both datasets:</p>
        
        <div class="equation">
            Y<sub>t</sub> = Œ≤‚ÇÄ + Œ≤‚ÇÅ √ó Y<sub>t-1</sub> + Œ≤‚ÇÇ √ó trend + Œµ<sub>t</sub>
        </div>
        
        <p>Where:</p>
        <ul>
            <li><strong>Y<sub>t</sub>:</strong> Current period value (CPI level or change)</li>
            <li><strong>Y<sub>t-1</sub>:</strong> Previous period value (lag term)</li>
            <li><strong>Œ≤‚ÇÄ:</strong> Intercept</li>
            <li><strong>Œ≤‚ÇÅ:</strong> Autoregressive coefficient</li>
            <li><strong>Œ≤‚ÇÇ:</strong> Trend coefficient</li>
            <li><strong>trend:</strong> Linear time trend (1, 2, 3, ...)</li>
        </ul>
        
        <h3>The Objective Function</h3>
        <p>We minimize the Mean Squared Error (MSE):</p>
        
        <div class="equation">
            Loss(Œ≤) = ¬Ω √ó Œ£(Y<sub>t</sub> - ≈∂<sub>t</sub>)¬≤
        </div>
        
        <h2><span class="emoji">‚ö†Ô∏è</span>The Numerical Problem</h2>
        
        <div class="problem-box">
            <h3>üî• The Condition Number Disaster</h3>
            <p>The condition number of X'X reveals the problem:</p>
            <ul>
                <li><strong>CPI Levels:</strong> ~10¬π‚Å∂ (catastrophically ill-conditioned)</li>
                <li><strong>CPI Changes:</strong> ~10¬≤ (well-conditioned)</li>
            </ul>
        </div>
        
        <p>When the condition number is very large, small changes in input can cause massive changes in output. This happens because:</p>
        
        <ol>
            <li><strong>Scale differences:</strong> CPI levels (250) vs trend values (1, 2, 3...) create vastly different magnitudes</li>
            <li><strong>Matrix ill-conditioning:</strong> The design matrix X becomes nearly singular</li>
            <li><strong>Gradient explosion:</strong> Optimization algorithms receive conflicting signals</li>
        </ol>
        
        <div class="code-block">
# What happens in the matrix:
X = [[1, 250.1, 1],      # Intercept, lag, trend
     [1, 251.3, 2],
     [1, 252.8, 3], ...]

# Condition number calculation:
XtX = X.T @ X
condition_number = np.linalg.cond(XtX)
# Result: ~10^16 for levels, ~10^2 for changes
        </div>
        
        <h2><span class="emoji">ü§ñ</span>The Optimization Algorithms</h2>
        
        <p>We test three adaptive optimizers to see how they handle the ill-conditioned problem:</p>
        
        <div class="optimizer-grid">
            <div class="optimizer-card">
                <h4>üîµ Adam Optimizer</h4>
                <p><strong>Strategy:</strong> Combines momentum with adaptive learning rates</p>
                <ul>
                    <li>Maintains moving averages of gradients and squared gradients</li>
                    <li>Includes bias correction</li>
                    <li>Generally robust across different problems</li>
                </ul>
            </div>
            
            <div class="optimizer-card">
                <h4>üü° RMSprop Optimizer</h4>
                <p><strong>Strategy:</strong> Adaptive learning rates without momentum</p>
                <ul>
                    <li>Uses moving average of squared gradients</li>
                    <li>No momentum component</li>
                    <li>Can be more sensitive to local variations</li>
                </ul>
            </div>
            
            <div class="optimizer-card">
                <h4>üü† AdaGrad Optimizer</h4>
                <p><strong>Strategy:</strong> Accumulates all past squared gradients</p>
                <ul>
                    <li>Becomes increasingly conservative over time</li>
                    <li>Learning rate decreases as optimization progresses</li>
                    <li>Can handle sparse gradients well</li>
                </ul>
            </div>
        </div>
        
        <h2><span class="emoji">üéØ</span>Learning Rate Adaptation</h2>
        
        <p>To make the comparison fair, we use different learning rates for each data type:</p>
        
        <div class="code-block">
# Learning rates tuned for each problem:
if optimizer_name == 'adam':
    levels_optimizer = AdamOptimizer(learning_rate=0.0001)  # Much smaller!
    changes_optimizer = AdamOptimizer(learning_rate=0.01)
elif optimizer_name == 'rmsprop':
    levels_optimizer = RMSpropOptimizer(learning_rate=0.001)
    changes_optimizer = RMSpropOptimizer(learning_rate=0.01)
elif optimizer_name == 'adagrad':
    levels_optimizer = AdaGradOptimizer(learning_rate=0.01)
    changes_optimizer = AdaGradOptimizer(learning_rate=0.1)
        </div>
        
        <div class="warning">
            <strong>‚ö†Ô∏è Critical Point:</strong> Even with careful learning rate tuning, the fundamental ill-conditioning of the levels problem remains. This demonstrates why data preprocessing and scaling are crucial in machine learning.
        </div>
        
        <h2><span class="emoji">üìà</span>The Optimization Animations</h2>
        
        <p>Watch how each optimizer handles the ill-conditioned CPI levels (left, red) versus the well-conditioned CPI changes (right, green). The animations show the optimization path in the Œ≤‚ÇÅ vs Œ≤‚ÇÇ parameter space:</p>
        
        <div class="optimizer-grid">
            <div class="optimizer-card">
                <h4>ü•á AdaGrad: The Consistent Champion</h4>
                <video width="100%" controls>
                    <source src="cpi_optimization_adagrad.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <ul>
                    <li><strong>CPI Levels:</strong> Smooth descent toward distant optimum (Œ≤‚ÇÅ ‚âà 1.0)</li>
                    <li><strong>CPI Changes:</strong> Perfect V-shaped convergence to the black star</li>
                    <li><strong>Final Loss:</strong> 63,900 (levels) / 15.9 (changes)</li>
                    <li><strong>Why it works:</strong> Conservative accumulated gradients prevent chaos</li>
                </ul>
            </div>
            
            <div class="optimizer-card">
                <h4>üîÑ Adam: The Momentum Maverick</h4>
                <video width="100%" controls>
                    <source src="cpi_optimization_adam.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <ul>
                    <li><strong>CPI Levels:</strong> Clean path but can't reach distant optimum</li>
                    <li><strong>CPI Changes:</strong> Oscillatory dance around the target</li>
                    <li><strong>Final Loss:</strong> 220,000 (levels) / 15.6 (changes) ‚≠êÔ∏è</li>
                    <li><strong>Why it struggles:</strong> Momentum causes overshoot on levels</li>
                </ul>
            </div>
            
            <div class="optimizer-card">
                <h4>üå™Ô∏è RMSprop: The Chaos Solver</h4>
                <video width="100%" controls>
                    <source src="cpi_optimization_rmsprop.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <ul>
                    <li><strong>CPI Levels:</strong> Diagonal descent finds excellent practical solution</li>
                    <li><strong>CPI Changes:</strong> Wild oscillations but surprisingly effective</li>
                    <li><strong>Final Loss:</strong> 39,200 (levels) ‚≠êÔ∏è / 674.4 (changes)</li>
                    <li><strong>Why it wins:</strong> Finds good solutions without reaching theoretical optima</li>
                </ul>
            </div>
        </div>
        
        <div class="results-section">
            <h3>üîç The Real Performance Story</h3>
            
            <p><strong>Expected:</strong> Clean paths = better performance</p>
            <p><strong>Reality:</strong> Path aesthetics ‚â† optimization quality!</p>
            
            <div class="insight">
                <strong>üéØ Key Revelations:</strong>
                <ul>
                    <li>The <strong>black stars</strong> show where algorithms should go (now visible with fixed scaling!)</li>
                    <li><strong>RMSprop's chaos</strong> on changes still produces decent results</li>
                    <li><strong>Adam's smooth levels path</strong> fails to reach the distant optimum</li>
                    <li><strong>AdaGrad's consistency</strong> makes it the most reliable across both problems</li>
                    <li>The <strong>scale differences</strong> (Œ≤‚ÇÅ: 0.1-1.0 vs 0.1-0.5) reveal the conditioning gap</li>
                </ul>
            </div>
            
            <div class="warning">
                <strong>üí° The Big Insight:</strong> Sometimes the algorithm that looks messy (RMSprop on changes) can still find excellent solutions, while the one that looks clean (Adam on levels) might not reach the target. This is why we measure final loss values, not just visual appeal!
            </div>
        </div>
        
        <h2><span class="emoji">üéì</span>The Broader Lessons</h2>
        
        <div class="insight">
            <h3>üíº For Machine Learning Practitioners:</h3>
            <ol>
                <li><strong>Data scaling is not optional:</strong> Always standardize or normalize your features</li>
                <li><strong>Check condition numbers:</strong> Use <code>np.linalg.cond()</code> to detect ill-conditioning</li>
                <li><strong>Algorithm choice matters:</strong> Some optimizers handle ill-conditioning better than others</li>
                <li><strong>Learning rates need careful tuning:</strong> Different scales require different learning rates</li>
                <li><strong>Empirical testing is crucial:</strong> Theoretical expectations don't always match reality</li>
            </ol>
        </div>
        
        <h2><span class="emoji">üîß</span>Practical Solutions</h2>
        
        <div class="solution-box">
            <h3>üõ†Ô∏è How to Fix the CPI Levels Problem:</h3>
            
            <div class="code-block">
# Option 1: Standardization
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Option 2: Log transformation
Y_log = np.log(Y / Y.shift(1))  # Convert to log returns

# Option 3: Differencing
Y_diff = Y.diff()  # First differences

# Option 4: Min-Max scaling
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_normalized = scaler.fit_transform(X)
            </div>
        </div>
        
        <h2><span class="emoji">üéØ</span>The Bottom Line</h2>
        
        <p>This visualization demonstrates a fundamental truth in machine learning: <span class="highlight">the same information presented at different scales can create completely different optimization challenges</span>. The CPI levels and changes represent identical economic data, yet one creates a numerical nightmare while the other optimizes smoothly.</p>
        
        <p>The key takeaway isn't just about CPI data - it's about the critical importance of data preprocessing in any machine learning pipeline. Without proper scaling, even the most sophisticated optimization algorithms can fail on perfectly solvable problems.</p>
        
        <div class="insight">
            <strong>üé™ The Real "Disaster":</strong> It's not the algorithms failing - it's what happens when we ignore the fundamental importance of data scaling in machine learning. The disaster is entirely preventable with proper preprocessing!
        </div>
    </div>
</body>
</html>
