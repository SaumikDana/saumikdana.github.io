<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CPI Time Series Optimization Problem</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=EB+Garamond&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.6;
            max-width: 1400px;
            margin: 0 auto;
            padding: 20px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: #333;
        }
        
        .container {
            background: white;
            border-radius: 15px;
            padding: 30px;
            box-shadow: 0 20px 40px rgba(0,0,0,0.1);
        }
        
        h1 {
            color: #2c3e50;
            text-align: center;
            font-size: 2.5em;
            margin-bottom: 10px;
            background: linear-gradient(45deg, #e74c3c, #27ae60);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }
        
        .subtitle {
            text-align: center;
            color: #7f8c8d;
            font-size: 1.2em;
            margin-bottom: 30px;
        }
        
        h2 {
            color: #34495e;
            border-left: 4px solid #3498db;
            padding-left: 15px;
            margin-top: 40px;
        }
        
        h3 {
            color: #2980b9;
            margin-top: 25px;
        }
        
        .data-section {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin: 20px 0;
        }
        
        .data-card {
            background: linear-gradient(135deg, #f8f9fa, #e9ecef);
            border-radius: 10px;
            padding: 20px;
            border-left: 5px solid #e74c3c;
        }
        
        .data-card.changes {
            border-left-color: #27ae60;
        }
        
        .equation {
            background: #2c3e50;
            color: white;
            padding: 15px;
            border-radius: 8px;
            font-family: 'Courier New', monospace;
            text-align: center;
            margin: 15px 0;
            font-size: 1.1em;
        }
        
        .problem-box {
            background: linear-gradient(135deg, #ff6b6b, #feca57);
            color: white;
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
            font-weight: bold;
        }
        
        .solution-box {
            background: linear-gradient(135deg, #48cae4, #023e8a);
            color: white;
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
        }
        
        .optimizer-grid {
            display: flex;
            flex-direction: column;
            gap: 30px;
            margin: 20px 0;
        }
        
        .optimizer-card {
            background: white;
            border: 2px solid #ecf0f1;
            border-radius: 10px;
            padding: 20px;
            transition: transform 0.2s;
        }
        
        .optimizer-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 25px rgba(0,0,0,0.1);
        }
        
        .optimizer-card h4 {
            color: #2980b9;
            margin-top: 0;
        }
        
        .code-block {
            background: #2d3748;
            color: #cbd5e0;
            padding: 15px;
            border-radius: 8px;
            overflow-x: auto;
            font-family: 'Courier New', monospace;
            margin: 15px 0;
            white-space: pre-wrap;
            word-wrap: break-word;
            line-height: 1.4;
        }
        
        .code-block code {
            display: block;
            white-space: pre-line;
            font-family: inherit;
        }
        
        .highlight {
            background: linear-gradient(120deg, #a8edea 0%, #fed6e3 100%);
            padding: 2px 6px;
            border-radius: 4px;
            font-weight: bold;
        }
        
        .results-section {
            background: linear-gradient(135deg, #667eea, #764ba2);
            color: white;
            padding: 25px;
            border-radius: 15px;
            margin: 30px 0;
        }
        
        .insight {
            background: #f39c12;
            color: white;
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
            border-left: 5px solid #e67e22;
        }
        
        .warning {
            background: #e74c3c;
            color: white;
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
            border-left: 5px solid #c0392b;
        }
        
        ul, ol {
            padding-left: 25px;
        }
        
        li {
            margin: 8px 0;
        }
        
        .emoji {
            font-size: 1.2em;
            margin-right: 8px;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>üö® CPI Time Series Optimization Disaster üö®</h1>
        <p class="subtitle">Why Data Scaling Matters in Machine Learning Optimization</p>
        
        <h2><span class="emoji">üìä</span>The Data: Two Faces of the Same Economic Story</h2>
        
        <p>We start with two CSV files containing Consumer Price Index (CPI) data - the same economic information presented in drastically different scales:</p>
        
        <div class="data-section">
            <div class="data-card">
                <h3>üî¥ CPI Levels (CPIAUCSL.csv)</h3>
                <ul>
                    <li><strong>Range:</strong> 78.0 to 307.5</li>
                    <li><strong>Scale:</strong> Large absolute numbers</li>
                    <li><strong>Example:</strong> 250.1, 251.3, 252.8...</li>
                    <li><strong>Problem:</strong> Creates numerical chaos</li>
                </ul>
            </div>
            
            <div class="data-card changes">
                <h3>üü¢ CPI Changes (CPIAUCSL_PCH.csv)</h3>
                <ul>
                    <li><strong>Range:</strong> -2.1% to +1.4%</li>
                    <li><strong>Scale:</strong> Small percentage changes</li>
                    <li><strong>Example:</strong> 0.3%, -0.1%, 0.7%...</li>
                    <li><strong>Problem:</strong> Well-behaved optimization</li>
                </ul>
            </div>
        </div>
        
        <div class="insight">
            <strong>üí° Key Insight:</strong> These datasets contain identical economic information! CPI changes are just the month-over-month percentage changes of CPI levels. Yet they create completely different optimization landscapes.
        </div>
        
        <h2><span class="emoji">üî¨</span>The Mathematical Setup</h2>
        
        <p>We create an autoregressive time series regression for both datasets:</p>
        
        <div class="equation">
            Y<sub>t</sub> = Œ≤‚ÇÄ + Œ≤‚ÇÅ √ó Y<sub>t-1</sub> + Œ≤‚ÇÇ √ó trend + Œµ<sub>t</sub>
        </div>
        
        <p>Where:</p>
        <ul>
            <li><strong>Y<sub>t</sub>:</strong> Current period value (CPI level or change)</li>
            <li><strong>Y<sub>t-1</sub>:</strong> Previous period value (lag term)</li>
            <li><strong>Œ≤‚ÇÄ:</strong> Intercept</li>
            <li><strong>Œ≤‚ÇÅ:</strong> Autoregressive coefficient</li>
            <li><strong>Œ≤‚ÇÇ:</strong> Trend coefficient</li>
            <li><strong>trend:</strong> Linear time trend (1, 2, 3, ...)</li>
        </ul>
        
        <h3>The Objective Function</h3>
        <p>We minimize the Mean Squared Error (MSE):</p>
        
        <div class="equation">
            Loss(Œ≤) = ¬Ω √ó Œ£(Y<sub>t</sub> - ≈∂<sub>t</sub>)¬≤
        </div>
        
        <h2><span class="emoji">‚ö†Ô∏è</span>The Numerical Problem</h2>
        
        <div class="problem-box">
            <h3>üî• The Condition Number Disaster</h3>
            <p>The condition number of X'X reveals the problem:</p>
            <ul>
                <li><strong>CPI Levels:</strong> ~10¬π‚Å∂ (catastrophically ill-conditioned)</li>
                <li><strong>CPI Changes:</strong> ~10¬≤ (well-conditioned)</li>
            </ul>
        </div>
        
        <p>When the condition number is very large, small changes in input can cause massive changes in output. This happens because:</p>
        
        <ol>
            <li><strong>Scale differences:</strong> CPI levels (250) vs trend values (1, 2, 3...) create vastly different magnitudes</li>
            <li><strong>Matrix ill-conditioning:</strong> The design matrix X becomes nearly singular</li>
            <li><strong>Gradient explosion:</strong> Optimization algorithms receive conflicting signals</li>
        </ol>
        
        
        <h2><span class="emoji">üìà</span>The Optimization Animations</h2>
        
        <p>Watch how each optimizer handles the ill-conditioned CPI levels (left, red) versus the well-conditioned CPI changes (right, green). The animations show the optimization path in the Œ≤‚ÇÅ vs Œ≤‚ÇÇ parameter space:</p>
        
        <div class="optimizer-grid">
            <div class="optimizer-card">
                <h4>ü•á AdaGrad: The Consistent Champion</h4>
                <video width="100%" controls>
                    <source src="cpi_optimization_adagrad.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <ul>
                    <li><strong>CPI Levels:</strong> Smooth descent toward distant optimum (Œ≤‚ÇÅ ‚âà 1.0)</li>
                    <li><strong>CPI Changes:</strong> Perfect V-shaped convergence to the black star</li>
                    <li><strong>Final Loss:</strong> 63,900 (levels) / 15.9 (changes)</li>
                    <li><strong>Why it works:</strong> Conservative accumulated gradients prevent chaos</li>
                </ul>
            </div>
            
            <div class="optimizer-card">
                <h4>üîÑ Adam: The Momentum Maverick</h4>
                <video width="100%" controls>
                    <source src="cpi_optimization_adam.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <ul>
                    <li><strong>CPI Levels:</strong> Clean path but can't reach distant optimum</li>
                    <li><strong>CPI Changes:</strong> Oscillatory dance around the target</li>
                    <li><strong>Final Loss:</strong> 220,000 (levels) / 15.6 (changes) ‚≠êÔ∏è</li>
                    <li><strong>Why it struggles:</strong> Momentum causes overshoot on levels</li>
                </ul>
            </div>
            
            <div class="optimizer-card">
                <h4>üå™Ô∏è RMSprop: The Chaos Solver</h4>
                <video width="100%" controls>
                    <source src="cpi_optimization_rmsprop.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <ul>
                    <li><strong>CPI Levels:</strong> Diagonal descent finds excellent practical solution</li>
                    <li><strong>CPI Changes:</strong> Wild oscillations but surprisingly effective</li>
                    <li><strong>Final Loss:</strong> 39,200 (levels) ‚≠êÔ∏è / 674.4 (changes)</li>
                    <li><strong>Why it wins:</strong> Finds good solutions without reaching theoretical optima</li>
                </ul>
            </div>
        </div>
        
        
        <h2><span class="emoji">üéì</span>The Broader Lessons</h2>
        
        <div class="insight">
            <h3>üíº For Machine Learning Practitioners:</h3>
            <ol>
                <li><strong>Data scaling is not optional:</strong> Always standardize or normalize your features</li>
                <li><strong>Check condition numbers:</strong> Use <code>np.linalg.cond()</code> to detect ill-conditioning</li>
                <li><strong>Algorithm choice matters:</strong> Some optimizers handle ill-conditioning better than others</li>
                <li><strong>Learning rates need careful tuning:</strong> Different scales require different learning rates</li>
                <li><strong>Empirical testing is crucial:</strong> Theoretical expectations don't always match reality</li>
            </ol>
        </div>
        
        <h2><span class="emoji">üîß</span>Practical Solutions</h2>
        
        <div class="solution-box">
            <h3>üõ†Ô∏è How to Fix the CPI Levels Problem:</h3>
            
            <div class="code-block">
<code># Option 1: Standardization
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Option 2: Log transformation
Y_log = np.log(Y / Y.shift(1))  # Convert to log returns

# Option 3: Differencing
Y_diff = Y.diff()  # First differences

# Option 4: Min-Max scaling
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_normalized = scaler.fit_transform(X)</code>
            </div>
        </div>
        
        <h2><span class="emoji">üéØ</span>The Bottom Line</h2>
        
        <p>This visualization demonstrates a fundamental truth in machine learning: <span class="highlight">the same information presented at different scales can create completely different optimization challenges</span>. The CPI levels and changes represent identical economic data, yet one creates a numerical nightmare while the other optimizes smoothly.</p>
        
        <p>The key takeaway isn't just about CPI data - it's about the critical importance of data preprocessing in any machine learning pipeline. Without proper scaling, even the most sophisticated optimization algorithms can fail on perfectly solvable problems.</p>
        
        <div class="insight">
            <strong>üé™ The Real "Disaster":</strong> It's not the algorithms failing - it's what happens when we ignore the fundamental importance of data scaling in machine learning. The disaster is entirely preventable with proper preprocessing!
        </div>
    </div>
</body>
</html>
