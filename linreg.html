<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Two-Level Reality of Regression</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=EB+Garamond&display=swap" rel="stylesheet">
    <script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$']],
            displayMath: [['$$', '$$']],
            processEscapes: true
        }
    };
    </script>
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js"></script>
    <style>
        body {
            font-family: Georgia, serif;
            line-height: 1.6;
            max-width: 1500px;
            margin: 0 auto;
            padding: 20px;
            background-color: #fafafa;
            color: #333;
        }
        .container {
            background: white;
            padding: 40px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 30px;
        }
        h2 {
            color: #34495e;
            margin-top: 30px;
            margin-bottom: 20px;
            border-left: 4px solid #3498db;
            padding-left: 15px;
        }
        h3 {
            color: #34495e;
            margin-top: 25px;
            margin-bottom: 15px;
        }
        ul {
            margin: 15px 0;
            padding-left: 25px;
        }
        li {
            margin: 8px 0;
        }
        .math-block {
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 5px;
            padding: 15px;
            margin: 20px 0;
            text-align: center;
        }
        .highlight {
            background: #fff3cd;
            border-left: 4px solid #ffc107;
            padding: 15px;
            margin: 20px 0;
        }
        .key-point {
            background: #d1ecf1;
            border-left: 4px solid #17a2b8;
            padding: 15px;
            margin: 20px 0;
        }
        strong {
            color: #2c3e50;
        }
        .section-divider {
            border: none;
            height: 2px;
            background: linear-gradient(to right, transparent, #3498db, transparent);
            margin: 40px 0;
        }
        .card { 
            border: 1px solid #e9ecef; 
            border-radius: 10px; 
            padding: 20px; 
            margin: 20px 0; 
            background: #fff; 
        }
        code, kbd, samp { 
            background: #f8f9fa; 
            padding: 2px 6px; 
            border-radius: 6px; 
        }
        .math { 
            font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", monospace; 
            background: #f8f9fa; 
            padding: 2px 6px; 
            border-radius: 6px; 
        }
        table { 
            border-collapse: collapse; 
            width: 100%; 
            margin: 20px 0; 
        }
        th, td { 
            border: 1px solid #e9ecef; 
            padding: 8px 12px; 
            text-align: right; 
        }
        th { 
            background: #f8f9fa; 
        }
        .muted {
            color: #6c757d;
            font-style: italic;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>The Two-Level Reality of Regression</h1>
        
        <p>Understanding regression requires distinguishing between what exists in reality versus what we can observe and estimate from our sample data.</p>

        <h2>Population Level (True but Unknown)</h2>
        
        <p>Simple linear regression assumes there's a true relationship in the population:</p>
        
        <div class="math-block">
            $$ y = \beta_0 + \beta_1x + \epsilon $$
        </div>
        
        <p><strong>Where:</strong></p>
        <ul>
            <li>$\beta_0$ = true population intercept (unknown parameter)</li>
            <li>$\beta_1$ = true population slope (unknown parameter)</li>
            <li>$\epsilon$ = random error term with mean 0</li>
        </ul>
        
        <div class="key-point">
            <p>These Greek letter parameters represent <em>reality</em>—the actual relationship that would exist if we could observe the entire population.</p>
        </div>

        <div class="math-block">
            $$\beta_1 = \rho_{XY} \frac{\sigma_Y}{\sigma_X}$$
        </div>

        <div class="math-block">
            $$\beta_0 = \mu_Y - \beta_1 \mu_X$$
        </div>

        <p><strong>Where:</strong></p>
        <ul>
            <li>$\rho_{XY} = \frac{\text{Cov}(X,Y)}{\sigma_X \sigma_Y}$ is the population correlation coefficient</li>
            <li>$\sigma_X = \sqrt{\text{Var}(X)}$ and $\sigma_Y = \sqrt{\text{Var}(Y)}$ are population standard deviations</li>
            <li>$\mu_X = E[X]$ and $\mu_Y = E[Y]$ are the population means</li>
        </ul>

        <hr class="section-divider">

        <h2>Sample Level (What We Estimate)</h2>
        
        <p>From our sample data, we estimate the population parameters using least squares:</p>
        
        <div class="math-block">
            $$ \hat{y} = \hat{\beta}_0 + \hat{\beta}_1x $$
        </div>
        
        <p><strong>Where:</strong></p>
        <ul>
            <li>$\hat{\beta}_0$ = sample estimate of the true intercept</li>
            <li>$\hat{\beta}_1$ = sample estimate of the true slope</li>
            <li>$\hat{y}$ = predicted value of $y$ for a given $x$</li>
        </ul>
        
        <div class="highlight">
            <p>The "hat" notation ($\hat{}$) always means "estimate of" or "predicted."</p>
        </div>

        <div class="math-block">
            $$ \hat{\beta}_1 = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2} $$
        </div>
        
        <div class="math-block">
            $$ \hat{\beta}_0 = \bar{y} - \hat{\beta}_1\bar{x} $$
        </div>

        <p><strong>Important distinction:</strong></p>
        <ul>
            <li>$\epsilon$: The true, unobservable error in the population model</li>
            <li>$e_i = y_i - \hat{y}_i$: The observable residuals from our fitted model</li>
        </ul>
        
        <p>Residuals are our best approximation of the true errors, but they're not the same thing.</p>

        <hr class="section-divider">

        <h2>Why We Care About the Distinction</h2>
        
        <p>This two-level framework matters because:</p>
        <ul>
            <li><strong>Inference</strong>: We want to make statements about $\beta_1$ (the true slope), not just $\hat{\beta}_1$ (our estimate)</li>
            <li><strong>Uncertainty</strong>: Our estimate $\hat{\beta}_1$ has sampling variability—different samples give different estimates</li>
            <li><strong>Hypothesis Testing</strong>: We test claims about the true parameter $\beta_1$, using our estimate $\hat{\beta}_1$</li>
        </ul>

        <h3>Standard Error of the Slope Estimate</h3>
        <div class="math-block">
            $$ SE(\hat{\beta}_1) = \sqrt{\frac{\frac{1}{n-2} \sum_{i=1}^n (y_i - \hat{y}_i)^2}{\sum_{i=1}^n (x_i - \bar{x})^2}} $$
        </div>

        <h3>t-Statistic for Testing $H_0: \beta_1 = 0$</h3>
        <div class="math-block">
            $$ t = \frac{\hat{\beta}_1}{SE(\hat{\beta}_1)} = \frac{\hat{\beta}_1}{\sqrt{\frac{\hat{\sigma}^2}{\sum_{i=1}^n (x_i - \bar{x})^2}}} $$
        </div>
        
        <p>This t-statistic follows a t-distribution with $(n-2)$ degrees of freedom under the null hypothesis.</p>

        <hr class="section-divider">

        <h2>Doubling the Data Points</h2>

        <h3>On $R^2$:</h3>
        <ul>
            <li>$R^2$ measures the proportion of variance explained: $R^2 = 1 - \frac{SSE}{SST}$</li>
            <li>Adding data doesn't automatically change $R^2$—it depends on whether new points follow the same pattern</li>
            <li>If new points are consistent with the existing relationship, $R^2$ may stay similar or slightly improve</li>
            <li>If new points are more scattered, $R^2$ could decrease</li>
        </ul>

        <h3>On p-values:</h3>
        <ul>
            <li>More data typically reduces $SE(\hat{\beta}_1)$ (denominator gets larger due to more spread in $x$-values)</li>
            <li>Smaller standard error means larger $|t|$-statistic for the same slope estimate</li>
            <li>Larger $|t|$-statistic means smaller p-value, stronger evidence against $H_0: \beta_1 = 0$</li>
        </ul>

        <h3>On the analytical solutions for $\beta_0$ and $\beta_1$:</h3>
        <ul>
            <li>Doubling the number of data points has <strong>no effect</strong> on the true population parameters themselves</li>
            <li>$\beta_1 = \frac{\text{Cov}(X,Y)}{\text{Var}(X)}$ remains constant—it's a fixed property of the population relationship</li>
            <li>$\beta_0 = \mu_Y - \beta_1 \mu_X$ also remains constant—it depends only on population moments</li>
            <li>The correlation $\rho_{XY}$ and population standard deviations $\sigma_X, \sigma_Y$ are unchanged</li>
        </ul>

        <h3>On the least squares estimates:</h3>
        <ul>
            <li>The <strong>actual values</strong> of $\hat{\beta}_0$ and $\hat{\beta}_1$ will change (new sample, new estimates)</li>
        </ul>

        <div class="key-point">
            <p><strong>Key Takeaway:</strong> More data gives us better estimates of the same underlying truth, not a different truth.</p>
        </div>

        <hr class="section-divider">

        <div class="card">
            <h2>SST, SSE, and R² — Tiny Worked Example</h2>
            <p>We have three data points: <span class="math">(x_i, y_i) = (1,1), (2,2), (3,2)</span>. We fit the simple OLS model <span class="math">$\hat{y} = \beta_0 + \beta_1 x$</span> and compute SST, SSE, and R² step by step.</p>
        </div>

        <div class="card">
            <h3>1) Mean of <span class="math">y</span></h3>
            <p><span class="math">$\bar{y} = (1 + 2 + 2)/3 = 5/3 \approx 1.6667$</span></p>
        </div>

        <div class="card">
            <h3>2) Total variation in <span class="math">y</span> (SST)</h3>
            <div class="math-block">
                $SST = \sum (y_i - \bar{y})^2$
            </div>
            <div class="math-block">
                $= (1 - 1.6667)^2 + (2 - 1.6667)^2 + (2 - 1.6667)^2$
                $= 0.4444 + 0.1111 + 0.1111 = 0.6667$
            </div>
        </div>

        <div class="card">
            <h3>3) Fit OLS line</h3>
            <p>Centered sums:</p>
            <ul>
                <li><span class="math">$\bar{x} = (1 + 2 + 3)/3 = 2$</span></li>
                <li><span class="math">$\sum (x_i-\bar{x})(y_i-\bar{y}) = 1.0$</span></li>
                <li><span class="math">$\sum (x_i-\bar{x})^2 = 2$</span></li>
            </ul>
            <p>Coefficients:</p>
            <div class="math-block">
                $\beta_1 = 1.0 / 2 = 0.5$,&nbsp;&nbsp;&nbsp;&nbsp;
                $\beta_0 = \bar{y} - \beta_1 \bar{x} = 1.6667 - 0.5 \cdot 2 = 0.6667$
            </div>
            <table>
                <thead>
                    <tr>
                        <th>x</th>
                        <th>y</th>
                        <th>$\hat{y} = 0.6667 + 0.5 \cdot x$</th>
                        <th>$y - \hat{y}$</th>
                        <th>$(y - \hat{y})^2$</th>
                    </tr>
                </thead>
                <tbody>
                    <tr><td>1</td><td>1.0000</td><td>1.1667</td><td>-0.1667</td><td>0.0278</td></tr>
                    <tr><td>2</td><td>2.0000</td><td>1.6667</td><td> 0.3333</td><td>0.1111</td></tr>
                    <tr><td>3</td><td>2.0000</td><td>2.1667</td><td>-0.1667</td><td>0.0278</td></tr>
                </tbody>
                <tfoot>
                    <tr>
                        <th colspan="4" style="text-align:right;">$SSE = \sum (y - \hat{y})^2$</th>
                        <th>0.1667</th>
                    </tr>
                </tfoot>
            </table>
        </div>

        <div class="card">
            <h3>4) Unexplained variation (SSE)</h3>
            <p><span class="math">$SSE = \sum (y_i - \hat{y}_i)^2 = 0.1667$</span></p>
        </div>

        <div class="card">
            <h3>5) R² (fraction of variance explained)</h3>
            <div class="math-block">
                $R^2 = 1 - \frac{SSE}{SST} = 1 - \frac{0.1667}{0.6667} = 0.75$
            </div>
            <p><strong>Interpretation:</strong> the regression explains <strong>75%</strong> of the variance in <span class="math">y</span>.</p>
            <p class="muted">
                Notes: If SSE = 0 ⇒ R² = 1 (perfect fit). If SSE ≈ SST ⇒ R² ≈ 0 (no explanatory power).
                If SSE > SST ⇒ R² < 0 (worse than predicting the mean).
            </p>
        </div>
    </div>
</body>
</html>
