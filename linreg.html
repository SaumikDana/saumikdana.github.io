<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Two-Level Reality of Regression</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=EB+Garamond&display=swap" rel="stylesheet">
    <script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$']],
            displayMath: [['$$', '$$']],
            processEscapes: true
        }
    };
    </script>
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js"></script>
    <style>
        body {
            font-family: Georgia, serif;
            line-height: 1.6;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #fafafa;
            color: #333;
        }
        .container {
            background: white;
            padding: 40px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 30px;
        }
        h2 {
            color: #34495e;
            margin-top: 30px;
            margin-bottom: 20px;
            border-left: 4px solid #3498db;
            padding-left: 15px;
        }
        h3 {
            color: #34495e;
            margin-top: 25px;
            margin-bottom: 15px;
        }
        ul {
            margin: 15px 0;
            padding-left: 25px;
        }
        li {
            margin: 8px 0;
        }
        .math-block {
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 5px;
            padding: 15px;
            margin: 20px 0;
            text-align: center;
        }
        .highlight {
            background: #fff3cd;
            border-left: 4px solid #ffc107;
            padding: 15px;
            margin: 20px 0;
        }
        .key-point {
            background: #d1ecf1;
            border-left: 4px solid #17a2b8;
            padding: 15px;
            margin: 20px 0;
        }
        strong {
            color: #2c3e50;
        }
        .section-divider {
            border: none;
            height: 2px;
            background: linear-gradient(to right, transparent, #3498db, transparent);
            margin: 40px 0;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>The Two-Level Reality of Regression</h1>
        
        <p>Understanding regression requires distinguishing between what exists in reality versus what we can observe and estimate from our sample data.</p>

        <h2>Population Level (True but Unknown)</h2>
        
        <p>Simple linear regression assumes there's a true relationship in the population:</p>
        
        <div class="math-block">
            $$ y = \beta_0 + \beta_1x + \epsilon $$
        </div>
        
        <p><strong>Where:</strong></p>
        <ul>
            <li>$\beta_0$ = true population intercept (unknown parameter)</li>
            <li>$\beta_1$ = true population slope (unknown parameter)</li>
            <li>$\epsilon$ = random error term with mean 0</li>
        </ul>
        
        <div class="key-point">
            <p>These Greek letter parameters represent <em>reality</em>—the actual relationship that would exist if we could observe the entire population.</p>
        </div>

        <div class="math-block">
            $$\beta_1 = \rho_{XY} \frac{\sigma_Y}{\sigma_X}$$
        </div>

        <div class="math-block">
            $$\beta_0 = \mu_Y - \beta_1 \mu_X$$
        </div>

        <p><strong>Where:</strong></p>
        <ul>
            <li>$\rho_{XY} = \frac{\text{Cov}(X,Y)}{\sigma_X \sigma_Y}$ is the population correlation coefficient</li>
            <li>$\sigma_X = \sqrt{\text{Var}(X)}$ and $\sigma_Y = \sqrt{\text{Var}(Y)}$ are population standard deviations</li>
            <li>$\mu_X = E[X]$ and $\mu_Y = E[Y]$ are the population means</li>
        </ul>

        <hr class="section-divider">

        <h2>Sample Level (What We Estimate)</h2>
        
        <p>From our sample data, we estimate the population parameters using least squares:</p>
        
        <div class="math-block">
            $$ \hat{y} = \hat{\beta}_0 + \hat{\beta}_1x $$
        </div>
        
        <p><strong>Where:</strong></p>
        <ul>
            <li>$\hat{\beta}_0$ = sample estimate of the true intercept</li>
            <li>$\hat{\beta}_1$ = sample estimate of the true slope</li>
            <li>$\hat{y}$ = predicted value of $y$ for a given $x$</li>
        </ul>
        
        <div class="highlight">
            <p>The "hat" notation ($\hat{}$) always means "estimate of" or "predicted."</p>
        </div>

        <div class="math-block">
            $$ \hat{\beta}_1 = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2} $$
        </div>
        
        <div class="math-block">
            $$ \hat{\beta}_0 = \bar{y} - \hat{\beta}_1\bar{x} $$
        </div>

        <p><strong>Important distinction:</strong></p>
        <ul>
            <li>$\epsilon$: The true, unobservable error in the population model</li>
            <li>$e_i = y_i - \hat{y}_i$: The observable residuals from our fitted model</li>
        </ul>
        
        <p>Residuals are our best approximation of the true errors, but they're not the same thing.</p>

        <hr class="section-divider">

        <h2>Why We Care About the Distinction</h2>
        
        <p>This two-level framework matters because:</p>
        <ul>
            <li><strong>Inference</strong>: We want to make statements about $\beta_1$ (the true slope), not just $\hat{\beta}_1$ (our estimate)</li>
            <li><strong>Uncertainty</strong>: Our estimate $\hat{\beta}_1$ has sampling variability—different samples give different estimates</li>
            <li><strong>Hypothesis Testing</strong>: We test claims about the true parameter $\beta_1$, using our estimate $\hat{\beta}_1$</li>
        </ul>

        <h3>Standard Error of the Slope Estimate</h3>
        <div class="math-block">
            $$ SE(\hat{\beta}_1) = \sqrt{\frac{\frac{1}{n-2} \sum_{i=1}^n (y_i - \hat{y}_i)^2}{\sum_{i=1}^n (x_i - \bar{x})^2}} $$
        </div>

        <h3>t-Statistic for Testing $H_0: \beta_1 = 0$</h3>
        <div class="math-block">
            $$ t = \frac{\hat{\beta}_1}{SE(\hat{\beta}_1)} = \frac{\hat{\beta}_1}{\sqrt{\frac{\hat{\sigma}^2}{\sum_{i=1}^n (x_i - \bar{x})^2}}} $$
        </div>
        
        <p>This t-statistic follows a t-distribution with $(n-2)$ degrees of freedom under the null hypothesis.</p>

        <hr class="section-divider">

        <h2>Doubling the Data Points</h2>

        <h3>On $R^2$:</h3>
        <ul>
            <li>$R^2$ measures the proportion of variance explained: $R^2 = 1 - \frac{SSE}{SST}$</li>
            <li>Adding data doesn't automatically change $R^2$—it depends on whether new points follow the same pattern</li>
            <li>If new points are consistent with the existing relationship, $R^2$ may stay similar or slightly improve</li>
            <li>If new points are more scattered, $R^2$ could decrease</li>
        </ul>

        <h3>On p-values:</h3>
        <ul>
            <li>More data typically reduces $SE(\hat{\beta}_1)$ (denominator gets larger due to more spread in $x$-values)</li>
            <li>Smaller standard error means larger $|t|$-statistic for the same slope estimate</li>
            <li>Larger $|t|$-statistic means smaller p-value, stronger evidence against $H_0: \beta_1 = 0$</li>
        </ul>

        <h3>On the analytical solutions for $\beta_0$ and $\beta_1$:</h3>
        <ul>
            <li>Doubling the number of data points has <strong>no effect</strong> on the true population parameters themselves</li>
            <li>$\beta_1 = \frac{\text{Cov}(X,Y)}{\text{Var}(X)}$ remains constant—it's a fixed property of the population relationship</li>
            <li>$\beta_0 = \mu_Y - \beta_1 \mu_X$ also remains constant—it depends only on population moments</li>
            <li>The correlation $\rho_{XY}$ and population standard deviations $\sigma_X, \sigma_Y$ are unchanged</li>
        </ul>

        <h3>On the least squares estimates:</h3>
        <ul>
            <li>The <strong>actual values</strong> of $\hat{\beta}_0$ and $\hat{\beta}_1$ will change (new sample, new estimates)</li>
        </ul>

        <div class="key-point">
            <p><strong>Key Takeaway:</strong> More data gives us better estimates of the same underlying truth, not a different truth.</p>
        </div>
    </div>
</body>
</html>
