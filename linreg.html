<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Two-Level Reality of Regression</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=EB+Garamond&display=swap" rel="stylesheet">
    <script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$']],
            displayMath: [['$$', '$$']],
            processEscapes: true
        }
    };
    </script>
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js"></script>
    <style>
        body {
            font-family: Georgia, serif;
            line-height: 1.6;
            max-width: 1500px;
            margin: 0 auto;
            padding: 20px;
            background-color: #fafafa;
            color: #333;
        }
        .container {
            background: white;
            padding: 40px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 30px;
        }
        h2 {
            color: #34495e;
            margin-top: 30px;
            margin-bottom: 20px;
            border-left: 4px solid #3498db;
            padding-left: 15px;
        }
        h3 {
            color: #34495e;
            margin-top: 25px;
            margin-bottom: 15px;
        }
        ul {
            margin: 15px 0;
            padding-left: 25px;
        }
        li {
            margin: 8px 0;
        }
        .math-block {
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 5px;
            padding: 15px;
            margin: 20px 0;
            text-align: center;
        }
        .highlight {
            background: #fff3cd;
            border-left: 4px solid #ffc107;
            padding: 15px;
            margin: 20px 0;
        }
        .key-point {
            background: #d1ecf1;
            border-left: 4px solid #17a2b8;
            padding: 15px;
            margin: 20px 0;
        }
        strong {
            color: #2c3e50;
        }
        .section-divider {
            border: none;
            height: 2px;
            background: linear-gradient(to right, transparent, #3498db, transparent);
            margin: 40px 0;
        }
        .card { 
            border: 1px solid #e9ecef; 
            border-radius: 10px; 
            padding: 20px; 
            margin: 20px 0; 
            background: #fff; 
        }
        code, kbd, samp { 
            background: #f8f9fa; 
            padding: 2px 6px; 
            border-radius: 6px; 
        }
        .math { 
            font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", monospace; 
            background: #f8f9fa; 
            padding: 2px 6px; 
            border-radius: 6px; 
        }
        table { 
            border-collapse: collapse; 
            width: 100%; 
            margin: 20px 0; 
        }
        th, td { 
            border: 1px solid #e9ecef; 
            padding: 8px 12px; 
            text-align: right; 
        }
        th { 
            background: #f8f9fa; 
        }
        .muted {
            color: #6c757d;
            font-style: italic;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>The Two-Level Reality of Regression</h1>
        
        <p>Understanding regression requires distinguishing between what exists in reality versus what we can observe and estimate from our sample data.</p>

        <h2>Population Level (True but Unknown)</h2>
        
        <p>Simple linear regression assumes there's a true relationship in the population:</p>
        
        <div class="math-block">
            $$ y = \beta_0 + \beta_1x + \epsilon $$
        </div>
        
        <p><strong>Where:</strong></p>
        <ul>
            <li>$\beta_0$ = true population intercept (unknown parameter)</li>
            <li>$\beta_1$ = true population slope (unknown parameter)</li>
            <li>$\epsilon$ = random error term with mean 0</li>
        </ul>
        
        <div class="key-point">
            <p>These Greek letter parameters represent <em>reality</em>—the actual relationship that would exist if we could observe the entire population.</p>
        </div>

        <div class="math-block">
            $$\beta_1 = \rho_{XY} \frac{\sigma_Y}{\sigma_X}$$
        </div>

        <div class="math-block">
            $$\beta_0 = \mu_Y - \beta_1 \mu_X$$
        </div>

        <p><strong>Where:</strong></p>
        <ul>
            <li>$\rho_{XY} = \frac{\text{Cov}(X,Y)}{\sigma_X \sigma_Y}$ is the population correlation coefficient</li>
            <li>$\sigma_X = \sqrt{\text{Var}(X)}$ and $\sigma_Y = \sqrt{\text{Var}(Y)}$ are population standard deviations</li>
            <li>$\mu_X = E[X]$ and $\mu_Y = E[Y]$ are the population means</li>
        </ul>

        <hr class="section-divider">

        <h2>Sample Level (What We Estimate)</h2>
        
        <p>From our sample data, we estimate the population parameters using least squares:</p>
        
        <div class="math-block">
            $$ \hat{y} = \hat{\beta}_0 + \hat{\beta}_1x $$
        </div>
        
        <p><strong>Where:</strong></p>
        <ul>
            <li>$\hat{\beta}_0$ = sample estimate of the true intercept</li>
            <li>$\hat{\beta}_1$ = sample estimate of the true slope</li>
            <li>$\hat{y}$ = predicted value of $y$ for a given $x$</li>
        </ul>
        
        <div class="highlight">
            <p>The "hat" notation ($\hat{}$) always means "estimate of" or "predicted."</p>
        </div>

        <div class="math-block">
            $$ \hat{\beta}_1 = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2} $$
        </div>
        
        <div class="math-block">
            $$ \hat{\beta}_0 = \bar{y} - \hat{\beta}_1\bar{x} $$
        </div>

        <p><strong>Important distinction:</strong></p>
        <ul>
            <li>$\epsilon$: The true, unobservable error in the population model</li>
            <li>$e_i = y_i - \hat{y}_i$: The observable residuals from our fitted model</li>
        </ul>
        
        <p>Residuals are our best approximation of the true errors, but they're not the same thing.</p>

        <hr class="section-divider">

        <h2>Why We Care About the Distinction</h2>
        
        <p>This two-level framework matters because:</p>
        <ul>
            <li><strong>Inference</strong>: We want to make statements about $\beta_1$ (the true slope), not just $\hat{\beta}_1$ (our estimate)</li>
            <li><strong>Uncertainty</strong>: Our estimate $\hat{\beta}_1$ has sampling variability—different samples give different estimates</li>
            <li><strong>Hypothesis Testing</strong>: We test claims about the true parameter $\beta_1$, using our estimate $\hat{\beta}_1$</li>
        </ul>

        <h3>Standard Error of the Slope Estimate</h3>
        <div class="math-block">
            $$ SE(\hat{\beta}_1) = \sqrt{\frac{\frac{1}{n-2} \sum_{i=1}^n (y_i - \hat{y}_i)^2}{\sum_{i=1}^n (x_i - \bar{x})^2}} $$
        </div>

        <h3>t-Statistic for Testing $H_0: \beta_1 = 0$</h3>
        <div class="math-block">
            $$ t = \frac{\hat{\beta}_1}{SE(\hat{\beta}_1)} = \frac{\hat{\beta}_1}{\sqrt{\frac{\hat{\sigma}^2}{\sum_{i=1}^n (x_i - \bar{x})^2}}} $$
        </div>
        
        <p>This t-statistic follows a t-distribution with $(n-2)$ degrees of freedom under the null hypothesis.</p>

        <hr class="section-divider">

        <h2>Doubling the Data Points</h2>

        <h3>On $R^2$:</h3>
        <ul>
            <li>$R^2$ measures the proportion of variance explained: $R^2 = 1 - \frac{SSE}{SST}$</li>
            <li>Adding data doesn't automatically change $R^2$—it depends on whether new points follow the same pattern</li>
            <li>If new points are consistent with the existing relationship, $R^2$ may stay similar or slightly improve</li>
            <li>If new points are more scattered, $R^2$ could decrease</li>
        </ul>

        <h3>On p-values:</h3>
        <ul>
            <li>More data typically reduces $SE(\hat{\beta}_1)$ (denominator gets larger due to more spread in $x$-values)</li>
            <li>Smaller standard error means larger $|t|$-statistic for the same slope estimate</li>
            <li>Larger $|t|$-statistic means smaller p-value, stronger evidence against $H_0: \beta_1 = 0$</li>
        </ul>

        <h3>On the analytical solutions for $\beta_0$ and $\beta_1$:</h3>
        <ul>
            <li>Doubling the number of data points has <strong>no effect</strong> on the true population parameters themselves</li>
            <li>$\beta_1 = \frac{\text{Cov}(X,Y)}{\text{Var}(X)}$ remains constant—it's a fixed property of the population relationship</li>
            <li>$\beta_0 = \mu_Y - \beta_1 \mu_X$ also remains constant—it depends only on population moments</li>
            <li>The correlation $\rho_{XY}$ and population standard deviations $\sigma_X, \sigma_Y$ are unchanged</li>
        </ul>

        <h3>On the least squares estimates:</h3>
        <ul>
            <li>The <strong>actual values</strong> of $\hat{\beta}_0$ and $\hat{\beta}_1$ will change (new sample, new estimates)</li>
        </ul>

        <div class="key-point">
            <p><strong>Key Takeaway:</strong> More data gives us better estimates of the same underlying truth, not a different truth.</p>
        </div>

        <hr class="section-divider">

        <div class="card">
            <h2>SST, SSE, and R² — Tiny Worked Example</h2>
            <p>We have three data points: <span class="math">(x_i, y_i) = (1,1), (2,2), (3,2)</span>. We fit the simple OLS model <span class="math">$\hat{y} = \beta_0 + \beta_1 x$</span> and compute SST, SSE, and R² step by step.</p>
        </div>

        <div class="card">
            <h3>1) Mean of <span class="math">y</span></h3>
            <p><span class="math">$\bar{y} = (1 + 2 + 2)/3 = 5/3 \approx 1.6667$</span></p>
        </div>

        <div class="card">
            <h3>2) Total variation in <span class="math">y</span> (SST)</h3>
            <div class="math-block">
                $SST = \sum (y_i - \bar{y})^2$
            </div>
            <div class="math-block">
                $= (1 - 1.6667)^2 + (2 - 1.6667)^2 + (2 - 1.6667)^2$
                $= 0.4444 + 0.1111 + 0.1111 = 0.6667$
            </div>
        </div>

        <div class="card">
            <h3>3) Fit OLS line</h3>
            <p>Centered sums:</p>
            <ul>
                <li><span class="math">$\bar{x} = (1 + 2 + 3)/3 = 2$</span></li>
                <li><span class="math">$\sum (x_i-\bar{x})(y_i-\bar{y}) = 1.0$</span></li>
                <li><span class="math">$\sum (x_i-\bar{x})^2 = 2$</span></li>
            </ul>
            <p>Coefficients:</p>
            <div class="math-block">
                $\beta_1 = 1.0 / 2 = 0.5$,&nbsp;&nbsp;&nbsp;&nbsp;
                $\beta_0 = \bar{y} - \beta_1 \bar{x} = 1.6667 - 0.5 \cdot 2 = 0.6667$
            </div>
            <table>
                <thead>
                    <tr>
                        <th>x</th>
                        <th>y</th>
                        <th>$\hat{y} = 0.6667 + 0.5 \cdot x$</th>
                        <th>$y - \hat{y}$</th>
                        <th>$(y - \hat{y})^2$</th>
                    </tr>
                </thead>
                <tbody>
                    <tr><td>1</td><td>1.0000</td><td>1.1667</td><td>-0.1667</td><td>0.0278</td></tr>
                    <tr><td>2</td><td>2.0000</td><td>1.6667</td><td> 0.3333</td><td>0.1111</td></tr>
                    <tr><td>3</td><td>2.0000</td><td>2.1667</td><td>-0.1667</td><td>0.0278</td></tr>
                </tbody>
                <tfoot>
                    <tr>
                        <th colspan="4" style="text-align:right;">$SSE = \sum (y - \hat{y})^2$</th>
                        <th>0.1667</th>
                    </tr>
                </tfoot>
            </table>
        </div>

        <div class="card">
            <h3>4) Unexplained variation (SSE)</h3>
            <p><span class="math">$SSE = \sum (y_i - \hat{y}_i)^2 = 0.1667$</span></p>
        </div>

        <div class="card">
            <h3>5) R² (fraction of variance explained)</h3>
            <div class="math-block">
                $R^2 = 1 - \frac{SSE}{SST} = 1 - \frac{0.1667}{0.6667} = 0.75$
            </div>
            <p><strong>Interpretation:</strong> the regression explains <strong>75%</strong> of the variance in <span class="math">y</span>.</p>
            <p class="muted">
                Notes: If SSE = 0 ⇒ R² = 1 (perfect fit). If SSE ≈ SST ⇒ R² ≈ 0 (no explanatory power).
                If SSE > SST ⇒ R² < 0 (worse than predicting the mean).
            </p>
        </div>

        <div class="card">
            <h3>6) F-statistic for testing overall model significance</h3>
            <p>The <strong>F-statistic</strong> tests whether the regression model explains a significant amount of variance compared to a model with no predictors (i.e., just the intercept).</p>
            
            <div class="math-block">
                $F = \frac{MSR}{MSE} = \frac{SSR/k}{SSE/(n-k-1)}$
            </div>
            
            <p><strong>Where:</strong></p>
            <ul>
                <li>$SSR$ = regression sum of squares (explained variation)</li>
                <li>$SSE$ = error sum of squares (unexplained variation)</li>
                <li>$k$ = number of predictors (excluding the intercept)</li>
                <li>$n$ = number of observations</li>
                <li>$MSR$ = mean square regression</li>
                <li>$MSE$ = mean square error</li>
            </ul>
            
            <p>Since $SSR = SST - SSE$, we can substitute:</p>
            <div class="math-block">
                $F = \frac{(SST - SSE)/k}{SSE/(n-k-1)}$
            </div>
            
            <p><strong>For our example:</strong></p>
            <ul>
                <li>$SST = 0.6667$</li>
                <li>$SSE = 0.1667$</li>
                <li>$SSR = SST - SSE = 0.6667 - 0.1667 = 0.5000$</li>
                <li>$k = 1$ (one predictor: $x$)</li>
                <li>$n = 3$ (three observations)</li>
            </ul>
            
            <div class="math-block">
                $F = \frac{0.5000/1}{0.1667/(3-1-1)} = \frac{0.5000}{0.1667/1} = \frac{0.5000}{0.1667} = 3.0$
            </div>
            
            <p><strong>Interpretation:</strong> This F-statistic of 3.0 (with 1 and 1 degrees of freedom) tests $H_0$: the regression model is no better than just predicting the mean $\bar{y}$ for all observations.</p>
            
            <p class="muted">
                Note: In simple linear regression, $F = t^2$ where $t$ is the t-statistic for testing $H_0: \beta_1 = 0$.
            </p>
        </div>

        <hr class="section-divider">

        <h2>Interpreting F-statistic Values</h2>
        
        <p>The F-statistic is fundamentally a ratio of signal to noise:</p>
        <div class="math-block">
            $F = \frac{\text{Signal (explained variance per predictor)}}{\text{Noise (unexplained variance per residual df)}}$
        </div>
        
        <p>Understanding what different F-values mean:</p>

        <div class="card">
            <h3>F ≈ 1 (close to 1)</h3>
            <ul>
                <li>The model's explained variance (SSR) is about the same order as the unexplained variance (SSE per degree of freedom)</li>
                <li>Adding predictors doesn't improve the model much beyond just predicting the mean</li>
                <li>Typically → <strong>not significant</strong></li>
            </ul>
        </div>

        <div class="card">
            <h3>F < 1 (very small)</h3>
            <ul>
                <li>The model actually explains <em>less variance</em> than would be expected by chance</li>
                <li>Strong evidence that predictors are useless or even harmful</li>
                <li>You'd fail to reject the null hypothesis (no relationship)</li>
            </ul>
        </div>

        <div class="card">
            <h3>F moderately large (say, > 4 or 5 depending on n, k)</h3>
            <ul>
                <li>Suggests predictors improve the model compared to noise</li>
                <li>You check the corresponding <strong>p-value</strong>:
                    <ul>
                        <li>If p < 0.05 → statistically significant (reject null)</li>
                        <li>If p is higher → still not significant, even if F > 1</li>
                    </ul>
                </li>
            </ul>
        </div>

        <div class="card">
            <h3>F very large (≫ 10, 20, 50...)</h3>
            <ul>
                <li>Strong evidence that predictors explain a lot of variance relative to noise</li>
                <li>P-value will usually be extremely small</li>
                <li>The regression model has strong explanatory power</li>
            </ul>
        </div>

        <div class="key-point">
            <h3>Quick Rule of Thumb</h3>
            <ul>
                <li><strong>Small F (≈0 → < 1):</strong> model worse than noise</li>
                <li><strong>Around 1:</strong> no improvement over mean-only model</li>
                <li><strong>Big F (≫1):</strong> model explains meaningful variance (significance depends on df & p-value)</li>
            </ul>
        </div>

        <div class="highlight">
            <p><strong>Why this makes sense:</strong> Under the null hypothesis (no real relationship), we'd expect F ≈ 1 on average because MSR would just reflect random variation in the predictors, while MSE reflects the true error variance. So the ratio should be around 1 if there's no real signal.</p>
            <p><strong>Remember:</strong> The degrees of freedom matter! An F of 4 might be significant with small sample sizes but not with very large ones, which is why we always check the p-value for formal hypothesis testing.</p>
        </div>

        <hr class="section-divider">

        <h2>What the F-statistic Actually Tests</h2>

        <p>The <strong>F-statistic</strong> in regression tests a very specific <strong>null hypothesis</strong>:</p>

        <div class="math-block">
            $H_0: \beta_1 = \beta_2 = \cdots = \beta_k = 0$
        </div>

        <p>(all slopes are zero at the same time)</p>

        <div class="key-point">
            <p><strong>What this means:</strong></p>
            <ul>
                <li>Under $H_0$, none of the independent variables help explain the dependent variable</li>
                <li>The model with predictors is no better than a simple mean-only model</li>
                <li>The regression equation reduces to just $\hat{y} = \hat{\beta}_0 = \bar{y}$</li>
            </ul>
        </div>

        <p>The <strong>alternative hypothesis</strong> is:</p>

        <div class="math-block">
            $H_A: \text{At least one } \beta_j \neq 0$
        </div>

        <div class="card">
            <h3>Decision Rule</h3>
            <ul>
                <li><strong>If F is large (and p-value small)</strong> → reject $H_0$. At least one predictor has explanatory power</li>
                <li><strong>If F is small</strong> → fail to reject $H_0$. The predictors (slopes) jointly have no explanatory power</li>
            </ul>
        </div>

        <div class="card">
            <h3>Important Nuances</h3>
            <ul>
                <li><strong>Joint test:</strong> The F-test examines whether ALL slopes are zero simultaneously, not individual slopes</li>
                <li><strong>Which predictors matter:</strong> Even if F is significant, it doesn't tell you WHICH predictors are important (you need individual t-tests for that)</li>
                <li><strong>Simple vs. multiple regression:</strong>
                    <ul>
                        <li>In simple linear regression (one predictor): $F = t^2$, so F-test and t-test are equivalent</li>
                        <li>In multiple regression: you could have significant F-test but some individual slopes not significant (and vice versa, though rarer)</li>
                    </ul>
                </li>
            </ul>
        </div>

        <div class="highlight">
            <p><strong>The Big Picture:</strong> The F-test compares your full regression model against the simplest possible model (just predicting the mean). It answers: "Is this collection of predictors, taken together, better than knowing nothing at all?"</p>
        </div>
    </div>
</body>
</html>
