<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Simplified Portfolio - Saumik Dana</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=EB+Garamond&display=swap" rel="stylesheet">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
    .content-container {
        display: flex;
        flex-direction: column;
        align-items: flex-start;
        gap: 20px;
    }

    .text-section, .image-section {
        width: 100%;
    }

    .image-section {
        flex-basis: auto;
    }

    .pointnet-image {
        margin-top: 0; /* Adjust this value as needed */
        width: 50%;
        height: auto;
        margin-bottom: 0; /* Adjust this value as needed */
    }
    
    .framework-steps {
        margin-top: 20px;
        padding: 15px;
        background-color: #f5f5f5;
        border-radius: 5px;
    }

    .framework-steps h2 {
        color: #333;
    }

    .framework-steps ol {
        padding-left: 20px;
    }

    .side-by-side-layout {
        display: flex;
        align-items: flex-start;
        gap: 20px;
    }

    .image-container {
        flex: 1;
        display: flex;
        flex-direction: column;
        align-items: center;
        gap: 10px;
    }

    .text-container {
        flex: 2;
    }

    .custom-image1, .custom-image2 {
        width: 100%;
        height: auto;
    }

    </style>
</head>
<body>
    <header>
    <img src="saumik.jpg" alt="Profile Picture of Saumik Dana" class="profile-pic">
    <div class="header-text">
    <h1>Point Cloud Processing with PointNet</h1>
    <a href="https://github.com/SaumikDana/DL_CV_Images" target="_blank" class="github-link">GitHub Repository</a>
    </div>
    <img src="montage2.webp" alt="PointNet" class="profile-pic-right">
    </header>
    <nav>
    </nav>
    <main>
    <p>
    During my recent stint at a surgical navigation startup, 
    while the company was cobbling a product demo, which I was heavily involved in, 
    I started looking at point clouds, and I embarked on a personal project to 
    leverage deep learning for point cloud processing. 
    </p>

    <section class="side-by-side-layout">
    <div class="image-container">
    <img src="fail_cnn.png" alt="PhD Work" class="custom-image1">
    <img src="fail_cnn_tmi.png" alt="PhD Work" class="custom-image2">
    </div>
    <div class="text-container">
    <h2>Problems with CNNs</h2>
    <blockquote>
    "Just like many other famous technologies in deep learning, 
    the main challenge of a technology in deep leaning is probably invariance. 
    Invariance is tightly related to efficiency, both in training speed and data. 
    For example, CNN has translational invariance or equivariance, 
    which means if we translate an object in an image from place A to place B, 
    the learned object feature is also translated to place B. 
    Thank to this property, we need not translate an object from places to places 
    for data augmentation. However, CNN cannot be rotation invariance. 
    That ‘s why we have to rotate the input images for data augmentation during training. 
    In point clouds, other than the above two invariances, 
    we have to consider one more: permutation invariance, 
    in which the learned point cloud features are consistent 
    no matter how the points are stored in the input file. 
    Therefore, the three invariance challenges are: 
    translation invariance, rotation invariance, and permutation invariance."
    </blockquote>
    </div>
    </section>

    <section class="side-by-side-layout">
    <div class="image-container">
    <img src="base_arch.png" alt="PhD Work" class="custom-image1">
    <img src="DL_PCD.png" alt="PhD Work" class="custom-image2">
    </div>
    <div class="text-container">
    <h2>PointNet Architecture</h2>
    <p>    
    Given the properties of point clouds, which the CNNs cannot satisfy, an architecture called PointNet
    was devised by researchers at Stanford in 2017. Multiple versions have been spawned off 
    this core architecture to perform tasks like segmentation, classification, and more 
    recently registration. 
    </p>
    <p>
    PointNetLK (PointNet Lucas-Kanade) is an adaptation and combination of the PointNet architecture 
    with the Lucas-Kanade algorithm for the task of 3D point cloud registration. 
    PointNet is a deep neural network designed to process point clouds (sets of points in a 3D space), 
    and the Lucas-Kanade method is a classical algorithm for image registration, 
    typically used for aligning images and tracking motion. 
    When adapted for 3D point cloud registration, 
    the goal is to align two sets of 3D points (point clouds) from different perspectives or times. 
    Here's an overall algorithmic framework for PointNetLK applied to 3D point cloud registration:
    <ul>
    <li><strong>Input Preparation:</strong>
    <ul>
    <li>Obtain two point clouds, a source and a target, that you want to align.</li>
    <li>Preprocess the point clouds if necessary (e.g., downsampling, denoising).</li>
    </ul>
    </li>
    <li><strong>Feature Extraction with PointNet:</strong>
    <ul>
    <li><strong>Source and Target Features:</strong> Pass both the source and target point clouds through a PointNet architecture to extract features.</li>
    <li><strong>Feature Representation:</strong> Obtain a global feature representation for each point cloud, capturing the distribution of points and their spatial relationships.</li>
    </ul>
    </li>
    <li><strong>Lucas-Kanade Iterative Alignment:</strong>
    <ul>
    <li><strong>Initial Parameters:</strong> Start with an initial guess of the transformation (e.g., identity if no prior knowledge).</li>
    <li><strong>Iterative Process:</strong>
    <ul>
    <li><strong>Warping:</strong> Apply the current estimate of the transformation to the source point cloud to align it with the target.</li>
    <li><strong>Error Computation:</strong> Compute the difference between the warped source and the target in the feature space provided by PointNet.</li>
    <li><strong>Parameter Update:</strong> Use the Lucas-Kanade method to update the transformation parameters to minimize this error.</li>
    </ul>
    </li>
    </ul>
    </li>
    <li><strong>Convergence Check:</strong>
    <ul>
    <li><strong>Termination Criteria:</strong> Check if the transformation parameters have converged or if a maximum number of iterations has been reached.</li>
    <li><strong>Output:</strong> If converged, return the final transformation parameters that best align the source to the target.</li>
    </ul>
    </li>
    <li><strong>Transformation Application:</strong>
    <ul>
    <li>Apply Final Transformation: Use the final estimated transformation to warp the source point cloud fully into the coordinate system of the target point cloud.</li>
    </ul>
    </li>
    </ul>
    </p>
    </div>
    </section>

    <section>
    <h2>Datasets</h2>
    <ul>
    <li><strong>3DMatch:</strong> <a href="https://3dmatch.cs.princeton.edu/" target="_blank">3DMatch Dataset</a>
    - Focused on 3D reconstruction and matching. 
    It is a collection of 3D scans used to benchmark algorithms 
    for 3D surface reconstruction and to align 3D scans 
    in the field of computer vision. The primary goal of 3DMatch 
    is to facilitate the development and testing of algorithms 
    that can match and align 3D data from different sources or viewpoints.</li>
    <li><strong>ShapeNet:</strong> <a href="https://shapenet.org/" target="_blank">ShapeNet</a>
    - Large-scale, richly-annotated dataset of 3D shapes. 
    It's designed for various tasks like object recognition and segmentation 
    in the realm of computer vision and AI.</li>
    </ul>    
    </section>
    </main>
    <footer>
        <p>Copyright © 2023 by Saumik Dana</p>
    </footer>
</body>
</html>
