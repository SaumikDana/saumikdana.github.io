<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Simplified Portfolio - Saumik Dana</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=EB+Garamond&display=swap" rel="stylesheet">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
    .content-container {
        display: flex;
        flex-direction: column;
        align-items: flex-start;
        gap: 20px;
    }

    .text-section, .image-section {
        width: 100%;
    }

    .image-section {
        flex-basis: auto;
    }

    .pointnet-image {
        margin-top: 0; /* Adjust this value as needed */
        width: 200%;
        max-width: 750px;
        height: auto;
        margin-bottom: -50px; /* Adjust this value as needed */
    }
    
    .framework-steps {
        margin-top: 20px;
        padding: 15px;
        background-color: #f5f5f5;
        border-radius: 5px;
    }

    .framework-steps h2 {
        color: #333;
    }

    .framework-steps ol {
        padding-left: 20px;
    }

    .side-by-side-container {
        display: flex;
        align-items: center;
        justify-content: space-between;
        gap: 20px;
    }

    .side-by-side-container .text-section, 
    .side-by-side-container .image-section {
        flex: 1;
    }

    .side-by-side-container .image-section img {
        width: 100%;
        height: auto;
    }

    </style>
</head>
<body>
    <header>
    <img src="saumik.jpg" alt="Profile Picture of Saumik Dana" class="profile-pic">
    <div class="header-text">
    <h1>Point Cloud Processing with PointNet</h1>
    <a href="https://github.com/SaumikDana/DL_CV_Images" target="_blank" class="github-link">GitHub Repository</a>
    </div>
    <img src="montage2.webp" alt="PointNet" class="profile-pic-right">
    </header>
    <nav>
    <a href="fail_cnn.html" class="tablinks">Problems with CNN</a>
    <a href="classification.html" class="tablinks">Classification</a>
    <a href="segmentation.html" class="tablinks">Segmentation</a>
    <a href="registration.html" class="tablinks">Registration</a>
    </nav>
    <main>
    <div class="content-container">
    <div class="side-by-side-container">
    <div class="text-section">
    <p>I embarked on an ambitious project to leverage deep learning 
    for point cloud processing. Utilizing the PointNet architecture, 
    I developed a model capable of handling complex spatial data. 
    This project was not only a technical challenge 
    but also a venture into optimizing computational resources 
    for real-time processing. The outcome was a robust model that significantly 
    enhanced the accuracy and efficiency of our point cloud analysis, 
    paving the way for more advanced applications in surgical navigation.</p>
    </div>
    <div class="image-section">
    <img src="DL_PCD.png" alt="Deep Learning on PCD" style="width: 75%;">
    </div>                
    </div>
    </div>

    Let's start building out the deep learning framework, by first attacking 
    the core architecture, and worrying about the training dataset later. 
    The core PointNet architecture is here:

    <div class="text-section">
    <img src="base_arch.png" alt="PointNet_Basic" class="pointnet-image">
    </div>

    Let's also make a list of all the datasets out there in the realm of PointNets applied
    to point cloud processing
    <ul>
    <li><strong>Princeton:</strong> <a href="https://3dmatch.cs.princeton.edu/" target="_blank">3DMatch Dataset</a>
    <ul>
    <li>Data: A 10,000x2 cell array of structs, each containing:
    <ul>
    <li>framePath: Path to the scene, sequence, and RGB-D frame</li>
    <li>pixelCoords: 1x2 array of pixel coordinates on the RGB-D frame</li>
    <li>camCoords: 3x1 array of 3D camera coordinates</li>
    <li>bboxCornersCam: 3x8 matrix of 3D camera coordinates for a 0.3m³ bounding box</li>
    <li>bboxRangePixels: 2x2 matrix of pixel corners of the projected bounding box</li>
    <li>camK: 3x3 matrix of camera intrinsics</li>
    <li>colorPatch: HxWx3 uint8 matrix of the RGB patch</li>
    <li>depthPatch: HxW matrix of the depth patch in meters</li>
    <li>voxelGridTDF: 30x30x30 matrix of TDF voxel grid values</li>
    </ul>
    </li>
    <li>Labels: In validation-set.mat only, a 10,000x1 cell array of binary correspondence labels</li>
    </ul>
    </li>
    <li><strong>Stanford:</strong> </li>
    </ul>
        
    </main>
    <footer>
    <p>Copyright © 2023 by Saumik Dana</p>
    </footer>
</body>

</html>
