<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Simplified Portfolio - Saumik Dana</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=EB+Garamond&display=swap" rel="stylesheet">  
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <header>
        <img src="saumik.jpg" alt="Profile Picture of Saumik Dana" class="profile-pic">
        <div class="header-text">
            <h1>Bayesian/MCMC on Time Series</h1>
        </div>
        <img src="montage3.webp" alt="Road Trips" class="profile-pic-right">
    </header>

    <main>
        <p>
            At USC, I spearheaded a project to develop a Bayesian inference 
            framework for model parameter estimation from time series data. 
            The challenge was to create a model that could accurately predict 
            and analyze complex data patterns. By employing Bayesian methods, 
            I was able to construct a framework that not only provided precise 
            estimations but also accounted for uncertainties in the data.
        </p>

        <h2>Bayesian Inference in Time Series Analysis</h2>
        <ul>
            <li><strong>Concept:</strong> Bayesian inference updates the probability of a model parameter's value based on observed time series data.</li>
            <li><strong>Process:</strong> It combines prior knowledge about the parameter (prior probability) with the likelihood of observing the data given the parameter values to produce an updated probability (posterior probability).</li>
            <li><strong>Bayes' Theorem:</strong> The process is governed by the formula \( P(\text{Parameter}|\text{Time Series}) = \frac{P(\text{Time Series}|\text{Parameter}) \times P(\text{Parameter})}{P(\text{Time Series})} \).</li>
            <li><strong>Application:</strong> This approach is particularly valuable in time series analysis for incorporating prior knowledge and handling complex, dynamic data.</li>
        </ul>

        <h2>Markov Chain Monte Carlo (MCMC) in Bayesian Time Series Analysis</h2>
        <ul>
            <li><strong>Purpose:</strong> MCMC is used to approximate the posterior distribution of a parameter when it's too complex to calculate directly in the context of time series data.</li>
            <li><strong>Method:</strong> It generates a series of samples through a Markov chain process, where the distribution of these samples converges to the posterior distribution of the parameter.</li>
            <li><strong>Algorithms:</strong> Includes techniques like Metropolis-Hastings and Gibbs sampling, tailored to explore the parameter space efficiently.</li>
            <li><strong>Utility:</strong> The samples from MCMC provide a way to estimate and understand the posterior distribution, allowing for predictions and uncertainty quantification in time series models.</li>
        </ul>

        <h2>Pseudocode</h2>
        <pre>
        # Define the prior distribution for the parameter
        function prior(parameter):
            # Define the prior probability of the parameter
            # This could be a uniform, normal, or any other distribution
            return probability_of_parameter

        # Define the likelihood function
        function likelihood(parameter, time_series_data):
            # Calculate the likelihood of the time series data given the parameter
            # This involves using your model to relate the parameter to the data
            return likelihood_of_data_given_parameter

        # Define the posterior distribution (unnormalized)
        function posterior(parameter, time_series_data):
            return likelihood(parameter, time_series_data) * prior(parameter)

        # MCMC Algorithm (e.g., Metropolis-Hastings)
        function MCMC(time_series_data, iterations):
            # Start with an initial guess for the parameter
            current_parameter = initial_guess
            samples = []

            for i in 1 to iterations:
                # Propose a new parameter value (this step depends on the specific MCMC algorithm)
                proposed_parameter = propose_new_parameter(current_parameter)

                # Calculate acceptance probability
                acceptance_probability = min(1, posterior(proposed_parameter, time_series_data) / posterior(current_parameter, time_series_data))

                # Accept or reject the new parameter value
                if random_uniform(0, 1) < acceptance_probability:
                    current_parameter = proposed_parameter

                # Collect samples after burn-in period
                if i > burn_in:
                    samples.append(current_parameter)

            return samples

        # Main procedure
        iterations = 10000
        burn_in = 1000
        time_series_data = load_your_time_series_data()

        # Run MCMC to sample from the posterior distribution
        posterior_samples = MCMC(time_series_data, iterations)

        # Analyze the posterior samples
        # e.g., calculate mean, median, credible intervals, etc.
        analyze_posterior_samples(posterior_samples)
        </pre>
    </main>

    <footer>
        <p>Copyright Â© 2023 by Saumik Dana</p>
    </footer>

</body>
</html>
